{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import copy\n",
    "\n",
    "SIZE = (210, 160, 3)\n",
    "NUM_OBS = 500\n",
    "NUM_EPISODES = 10\n",
    "WINDOW_SIZE = 3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up an LSTM Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, inp, hidden, layers):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden #defining no. of hidden Layers\n",
    "        self.layers = layers #Defining no. of layers in an LSTM\n",
    "        self.lstm = nn.LSTM(inp, hidden, layers,batch_first=True) #giving arguments as input to the LSTM\n",
    "        self.fc = nn.Linear(hidden, 7) #Setting prediction to be of siongle dimension\n",
    " \n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.layers, batch_size, self.hidden).to(device) #Initializing\n",
    "        c0 = torch.zeros(self.layers, batch_size, self.hidden).to(device) #Initializing\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :]) #Taking the final time_step of the time sequence\n",
    "        return out\n",
    " \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Conv2d(3,8,5,bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.MaxPool2d(4,4),\n",
    " \n",
    " \n",
    "            nn.Conv2d(8,32,3,bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.MaxPool2d(2,2),\n",
    " \n",
    "            nn.Conv2d(32,64,3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.MaxPool2d(2,2),\n",
    " \n",
    "            nn.Conv2d(64,64,3,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    " \n",
    "            nn.MaxPool2d((2,3),2),\n",
    "            # nn.Conv2d(64, 64, 3, bias=False, padding=1),\n",
    "            # nn.ReLU(),\n",
    " \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.fc(x)\n",
    "        x=x.view(-1,512)\n",
    "        return x\n",
    " \n",
    " \n",
    "class RL_LSTM_Q_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = Net()\n",
    "        self.lstm = LSTM(512, 16, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch_size = batch.size(0)\n",
    "        window_size = batch.size(1)\n",
    "        re_batch = batch.reshape(-1,3,210,160)\n",
    "        out = self.conv(re_batch)\n",
    "        out = out.reshape(batch_size,window_size,512)\n",
    "        out_2 = self.lstm(out)\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, model, deepMem, epsMem, batchsize=64, epsize=3, l_rate=0.001):\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "        self.deepMem = deepMem\n",
    "        self.epsMem = epsMem\n",
    "        self.batchsize = batchsize\n",
    "        self.epsize = epsize\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = 0.9\n",
    "        self.optimizer = torch.optim.Adagrad(self.model.parameters(), lr=l_rate)\n",
    "        self.train_count = 0\n",
    "\n",
    "\n",
    "    def process_img(self, img):\n",
    "        img = img / 255.0\n",
    "        img = img - 0.445 # mean\n",
    "        img = img / 0.225 # std\n",
    "        img = np.transpose(img, axes=(2, 0, 1))\n",
    "        return img\n",
    "\n",
    "    def parse_state(self, state):\n",
    "        new_state = [self.process_img(x) for x in state]\n",
    "        return new_state\n",
    "    \n",
    "    def make_states(self, states):\n",
    "        states = np.array([self.parse_state(s) for s in states])\n",
    "        states = torch.Tensor(states).view(-1, WINDOW_SIZE, 3, 210, 160).to(device)\n",
    "        return states\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = self.make_states(state)\n",
    "        pred = self.model(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            self.epsilon = self.epsilon * 0.999\n",
    "            return random.randint(0, 6)\n",
    "        else:\n",
    "            return torch.argmax(pred[0]).item()\n",
    "\n",
    "    def train(self):\n",
    "        deep_batch = self.deepMem.sample(self.batchsize)\n",
    "        eps_batch = self.epsMem.sample(self.epsize)\n",
    "        if(len(eps_batch) > self.batchsize):\n",
    "            rindex = random.randint(0, len(eps_batch) - self.batchsize)\n",
    "            eps_batch = eps_batch[rindex:][:self.batchsize]\n",
    "        batch = deep_batch + eps_batch\n",
    "        \n",
    "        states = [x[0] for x in batch]\n",
    "        next_states = [x[1] for x in batch]\n",
    "        actions = [int(x[2]) for x in batch]\n",
    "        rewards = [float(x[3]) for x in batch]\n",
    "        dones = [float(int(x[4])) for x in batch]\n",
    "        \n",
    "        states = self.make_states(states)\n",
    "        next_states = self.make_states(next_states)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        future_reward = torch.max(self.target_model(next_states), 1)[0]\n",
    "        dones = torch.Tensor(dones).to(device)\n",
    "        rewards = torch.Tensor(rewards).to(device)\n",
    "        actions = torch.Tensor(actions).long().to(device)\n",
    "\n",
    "        final_reward = (rewards + future_reward * (1.0 - dones) * self.gamma).detach()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        predicted_reward = self.model(states)\n",
    "        actions_one_hot = torch.nn.functional.one_hot(actions, 7)\n",
    "        predicted_reward = torch.sum(predicted_reward * actions_one_hot, axis=1)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(predicted_reward, final_reward)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.train_count += 1\n",
    "\n",
    "        if self.train_count % 10 == 0:\n",
    "            self.target_model = copy.deepcopy(self.model)\n",
    "            \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "class DeepMemory:\n",
    "    def __init__(self, MAX_LENGTH=5000):\n",
    "        self.memory = {}\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "    \n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        if reward not in self.memory:\n",
    "            self.memory[reward] = []\n",
    "        \n",
    "        self.memory[reward].append((state, next_state, action, reward, done))\n",
    "        \n",
    "        if len(self.memory[reward]) > self.MAX_LENGTH:\n",
    "            self.memory[reward].pop(0)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        for i in range(batch_size):\n",
    "            reward = np.random.choice(list(self.memory.keys()))\n",
    "            batch.append(random.choice(self.memory[reward]))\n",
    "        return batch\n",
    "\n",
    "    def render_sample(self, sample):\n",
    "        for el in sample:\n",
    "            state, next_state, action, reward, done = el\n",
    "            \n",
    "            print(\"--------------------------------------------------\")\n",
    "            plt.title(\"Curr State\")\n",
    "            plt.imshow(np.hstack(state))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            plt.title(\"Next State\")\n",
    "            plt.imshow(np.hstack(next_state))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Action: \", action)\n",
    "            print(\"Reward: \", reward)\n",
    "            print(\"Done: \", done)\n",
    "            print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicMemory:\n",
    "    def __init__(self, MAX_LENGTH=5000):\n",
    "        self.memory = []\n",
    "        self.episode_memory = []\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "    \n",
    "    def start_episode(self):\n",
    "        self.episode_memory = []\n",
    "    \n",
    "    def end_episode(self):\n",
    "        self.memory.append(self.episode_memory.copy())\n",
    "        if len(self.memory) > self.MAX_LENGTH:\n",
    "            self.memory.pop(0)\n",
    "    \n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        self.episode_memory.append((state, next_state, action, reward, done))\n",
    "    \n",
    "    def sample(self, episode_size):\n",
    "        batch = []\n",
    "        if(len(self.memory) == 0):\n",
    "            return batch\n",
    "        for _ in range(episode_size):\n",
    "            index = np.random.randint(0, len(self.memory))\n",
    "            batch = batch + self.memory[index]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frames(frames):\n",
    "    plt.imshow(np.hstack(frames))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_rewards(rew):\n",
    "    ax = plt.figure(figsize=(6, 3))\n",
    "    plt.plot(rew)\n",
    "    plt.xlabel(\"Time step in Frames\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "    \n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "env = gym.make(\"ALE/Assault-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "deepMem = DeepMemory(MAX_LENGTH=20)\n",
    "epMem = EpisodicMemory(MAX_LENGTH=2)\n",
    "model = RL_LSTM_Q_Network().to(device)\n",
    "agent = Agent(model, deepMem, epMem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    observation, info = env.reset()\n",
    "    frames = []\n",
    "    obs, rew = [], []\n",
    "    curr_state = [np.zeros(SIZE, dtype=np.uint8) for i in range(WINDOW_SIZE)]\n",
    "    total_reward = 0\n",
    "    epMem.start_episode()\n",
    "    \n",
    "    for t in tqdm(range(200)):\n",
    "        frames.append(env.render())\n",
    "        action = agent.predict(np.array([curr_state]))\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        prev_state = curr_state.copy()\n",
    "        curr_state.pop(0)\n",
    "        curr_state.append(observation.copy())\n",
    "        obs.append(observation.copy())\n",
    "        rew.append(reward)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        deepMem.remember(prev_state, curr_state.copy(), action, reward, done)\n",
    "        epMem.remember(prev_state, curr_state.copy(), action, reward, done)\n",
    "        agent.train()\n",
    "        if done:\n",
    "            break\n",
    "    epMem.end_episode()\n",
    "\n",
    "    \n",
    "    if(i % 1 == 0):\n",
    "        save_frames_as_gif(frames)\n",
    "        print(\"Episode: \", i)\n",
    "        display(Image(data=open('gym_animation.gif','rb').read(), format='png'))\n",
    "        print(\"Total Reward: \", total_reward)\n",
    "        print(\"--------------------------------------------------\")\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
